---
title: "Electricity Consumption analysis"
author: "Group 1"
date: "3/24/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Library

```{r message=FALSE, warning=FALSE}
# load packages :
library(tidyverse)
library(lubridate)
library(janitor)
library(forecast)
library(imputeTS)
library(hts)
library(MAPA)
library(thief)
```

## Data preparation

We first prepare the data to generate 2 sets of data\
1. load_ts1 (4-weeks of data) \
2. load_ts2 (5-months of data)

Please note that currently I have imputed 4-weeks + 8 days to mimic the submission

```{r message=FALSE, warning=FALSE}
# load packages :
library(tidyverse)
library(lubridate)
library(janitor)
library(forecast)
library(imputeTS)
library(hts)
library(MAPA)
library(thief)


# set working directory :
setwd("C:/Users/klchi/OneDrive/Desktop/Advance business forecast/Mar22")

# country initials :
country = c("BE","FI","FR","DE","GR","IT","PO","RO","ES","SE")



###### 2 WEEKS OF DATA ######
load_data1 <- list()

# initialize the time frame that will be used to train the model :
start_date = "2022-02-17" # 2022-02-18  2022-03-10    2022-04-14
end_date = "2022-03-24"   # 2022-03-03  2022-03-24    2022-04-28

for (i in 1:length(country)) {
  # 2021 data:
  x1 <- read.csv(paste0(country[i],"-2021.csv")) %>% select(1,3)
  names(x1) <- c("Date_Time","Load")
  x1 <- x1 %>% 
    mutate(Load = as.numeric(Load),
           Date = dmy(gsub("\\.","\\-",str_sub(Date_Time,1,10))),
           Time = paste0(str_sub(Date_Time,12,16),"-",
                         str_sub(Date_Time,-5,-1))) %>% 
    filter(Date >= start_date) %>% 
    select(3,4,2)
  
  # 2022 data:
  x2 <- read_csv(paste0(country[i],"-2022.csv")) %>% select(1,3)
  names(x2) <- c("Date_Time","Load")
  x2 <- x2 %>% 
    mutate(Load = case_when(
      # is.na(Load) ~ 0,
      Load=="-" ~ "0",
      Load=="" ~ "0",
      Load=="N/A" ~ "NA",
      TRUE ~ Load
    ),
    Date = dmy(gsub("\\.","\\-",str_sub(Date_Time,1,10))),
    Time = paste0(str_sub(Date_Time,12,16),"-",
                  str_sub(Date_Time,-5,-1))) %>% 
    filter(Date <= end_date) %>% 
    filter(Date >= start_date) %>% 
    select(3,4,2) %>% 
    mutate(Load=as.numeric(Load))
  
  # merge 2011 and 2022 data together :
  x <- rbind(x1,x2) %>% 
    mutate(Time=str_sub(Time,1,5))
  
  # if we have data per 15':
  if (i %in% c(1,4,8)) {
    x <- x %>% group_by(count = (row_number() -1) %/% 4) %>%
      mutate(xLoad = round(sum(Load,na.rm=T),0)) %>% 
      select(1,2,5) %>% 
      rename(Load=xLoad) %>% 
      tibble() %>% 
      select(-1)
    x <- x[seq(1,dim(x)[1],4),] 
  }
  
  # data frame in its final form :
  x <- x %>% 
    mutate(Date_Time = paste0(Date," ",Time,":00")) %>% 
    select(4,3)
  load_data1[[i]] <- data.frame(x)
  load_data1[[i]] <- load_data1[[i]][!duplicated(load_data1[[i]]$Date_Time),]
  
}
# name every data frame by country initials :
names(load_data1) <- country

# check if there are any missing values (NA) : 
lapply(load_data1,function(x) {colSums(is.na(x))})

# treat missing values:
# 3, 5, 8, 9 have missing values. use imputeTS package for imputation:
# here, we applied simple moving average:
for (i in c(3,5,8,9)) {
  load_data1[[i]]$Load <- na_ma(load_data1[[i]]$Load,weighting = "simple")
}

# create a list to hold time series data :
load_ts1 <- list()
for (i in 1:length(load_data1)) {
  load_data1[[i]]$Date_Time <- ymd_hms(load_data1[[i]]$Date_Time)
  load_ts1[[i]] <- load_data1[[i]]$Load %>% ts(freq=24)
}
# name every time series by country initials :
names(load_ts1) <- country



###### 5 MONTHS OF DATA ######
load_data2 <- list()

# initialize the time frame that will be used to train the model :
start_date = "2021-10-24" # 2021-10-03  2021-10-24    2021-11-28
end_date = "2022-03-24"   # 2022-03-03  2022-03-24    2022-04-28

for (i in 1:length(country)) {
  # 2021 data:
  x1 <- read.csv(paste0(country[i],"-2021.csv")) %>% select(1,3)
  names(x1) <- c("Date_Time","Load")
  x1 <- x1 %>% 
    mutate(Load = as.numeric(Load),
           Date = dmy(gsub("\\.","\\-",str_sub(Date_Time,1,10))),
           Time = paste0(str_sub(Date_Time,12,16),"-",
                         str_sub(Date_Time,-5,-1))) %>% 
    filter(Date >= start_date) %>%  
    select(3,4,2)
  
  # 2022 data:
  x2 <- read_csv(paste0(country[i],"-2022.csv")) %>% select(1,3)
  names(x2) <- c("Date_Time","Load")
  x2 <- x2 %>% 
    mutate(Load = case_when(
      # is.na(Load) ~ 0,
      Load=="-" ~ "0",
      Load=="" ~ "0",
      Load=="N/A" ~ "NA",
      TRUE ~ Load
    ),
    Date = dmy(gsub("\\.","\\-",str_sub(Date_Time,1,10))),
    Time = paste0(str_sub(Date_Time,12,16),"-",
                  str_sub(Date_Time,-5,-1))) %>% 
    filter(Date <= end_date) %>% 
    filter(Date >= start_date) %>% 
    select(3,4,2) %>% 
    mutate(Load=as.numeric(Load))
  
  # merge 2011 and 2022 data together :
  x <- rbind(x1,x2) %>% 
    mutate(Time=str_sub(Time,1,5))
  
  # if we have data per 15':
  if (i %in% c(1,4,8)) {
    x <- x %>% group_by(count = (row_number() -1) %/% 4) %>%
      mutate(xLoad = round(sum(Load,na.rm=T),0)) %>% 
      select(1,2,5) %>% 
      rename(Load=xLoad) %>% 
      tibble() %>% 
      select(-1)
    x <- x[seq(1,dim(x)[1],4),] 
  }
  
  # data frame in its final form :
  x <- x %>% 
    mutate(Date_Time = paste0(Date," ",Time,":00")) %>% 
    select(4,3)
  load_data2[[i]] <- data.frame(x)
  load_data2[[i]] <- load_data2[[i]][!duplicated(load_data2[[i]]$Date_Time),]
  
}
# name every data frame by country initials :
names(load_data2) <- country

# check if there are any missing values (NA) : 
lapply(load_data2,function(x) {colSums(is.na(x))})

# treat missing values and 0s:
# 3, 5, 8, 9 have missing values. use imputeTS package for imputation:
# here, we applied simple moving average:
for (i in c(3,5,8,9)) {
  load_data2[[i]]$Load <- na_ma(load_data2[[i]]$Load,weighting = "simple")
}

for (i in 1:dim(load_data2[[1]])[1]) {
 for (j in 1:length(load_data2)) {
   if (load_data2[[j]]$Load[i] == 0) {
     load_data2[[j]]$Load[i] = mean(load_data2[[j]]$Load)
   }
 }
}

# create a list to hold time series data :
load_ts2 <- list()
for (i in 1:length(load_data2)) {
  load_data2[[i]]$Date_Time <- ymd_hms(load_data2[[i]]$Date_Time)
  load_ts2[[i]] <- load_data2[[i]]$Load %>% ts(freq=24)
}
# name every time series by country initials :
names(load_ts2) <- country

# check for outliers (replacements are made using linear interpolation):
for (i in 1:length(load_ts1)) {
  load_ts1[[i]] <- tsclean(load_ts1[[i]])
}
for (i in 1:length(load_ts2)) {
  load_ts2[[i]] <- tsclean(load_ts2[[i]])
}

# clear environment :
# keep 2 week and 5 month datasets:
rm(list=setdiff(ls(),c("load_ts1","load_ts2"))) #"load_data1","load_data2"
```

## Model Selection

Findthe best models out of below selection:

1.  MSTS

2.  Tbats

3.  MAPA

4.  THIEF

Please note here we are only using the first 4 weeks of data in load_ts1 for model selection.

```{r echo=FALSE, message=FALSE, warning=FALSE}
best <- array(NA,length(load_ts1))

# for every time series:
for (i in 1:length(load_ts1)){
  #Only use the first 4 weeks of data
  y <- head(load_ts1[[i]],4*7*24)
  dy <- data.frame(y=as.vector(y),x=rep(0:23,length(y)/24))
  
  # split the data to train/validation sets:
  h <- 7*24 
  yt <- head(y, length(y) - h)
  yv <- tail(y, h)
  xt <- head(dy$x, length(y) - h)
  xv <- tail(dy$x, h)
  yt_thief <-ts(yt,freq=24)
  
  # multi-seasonal time series format, train-validation split:
  yms <- msts(y,seasonal.periods = c(24,24*7))
  ymst <- head(yms, length(yms)-h)
  ymsv <- tail(yms, h)
  
  # MODEL POOL
  models <- c("msts","tbats","thief","mapa",
              "determ","stoch","dynam-harm-","ae-comb")
  
  ### Multiple seasonalities (daily, hourly)
  fit1 <- stlm(ymst, lambda = 0)
  fc1 <- forecast(fit1, h=h)$mean
  
  ### TBATS with time of day as x-variable:
  fit2 <- tbats(log(ymst),use.box.cox=F,use.trend = TRUE,
                use.damped.trend = TRUE,xreg=xt)
  fc2 <- exp(forecast(fit2, h=h)$mean)
  
  ### Thief 
  fit3 <- thief(yt_thief,h=h,comb="struc", usemodel="ets")
  fc3 <- forecast(fit3, h=h)$mean
  
  ### MAPA
  fc4 <- mapasimple(yt, comb = "w.mean", fh = h)
  
  ### deterministic trend:
  # fit5 <- auto.arima(yt, d=0, xreg=1:length(yt))
  # fc5 <- forecast(fit5, xreg=length(yt)+1:h, h=h)$mean
  
  ### stochastic trend:
  # fit6 <- auto.arima(yt, d=1)
  # fc6 <- forecast(fit6, h=h)$mean
  
  ### dynamic-harmonic regression:
  # AICc <- array(NA,6)
  # for (j in 1:6) {
  #   x <- auto.arima(yt, xreg = fourier(yt, K = j), seasonal=FALSE, lambda=0)
  #   AICc[[j]] <- x[["aicc"]] 
  # }
  # models[7] <- paste0(models[7],which.min(AICc))
  # fit7 <- auto.arima(yt, xreg = fourier(yt, K=which.min(AICc)),
  #                    seasonal = FALSE, lambda = 0)
  # fc7 <- forecast(fit7,xreg=fourier(yt, K=which.min(AICc), h=h))$mean
  
  ### combination of ets-arima models:
  # fc8 <- 0.5*as.vector(forecast(auto.arima(yt),h=h)$mean) +
  #        0.5*as.vector(forecast(ets(yt),h=h)$mean)
  
  # evaluate accuracy using MAPE:
  # ?forecast::accuracy()
  MAPEs <- round(c(accuracy(fc1,ymsv)[5], accuracy(fc2,ymsv)[5],
                   accuracy(fc3,yv)[5], accuracy(fc4$forecast,yv)[5]),2)
  #                accuracy(fc5,yv)[5], accuracy(fc6,yv)[5],
  #                accuracy(fc7,yv)[5], accuracy(fc8,yv)[5]
  
  # choose model based on better MAPE value:
  best[i] <- models[which.min(MAPEs)]
}
```

Below are the results of the model selection

```{r}
best
```

## Period Selection

below code is used to generate the what period is used for the selected best model

1.  4-weeks

2.  5-months

3.  Combine model (0.5\*4-weeks + 0.5\*5-months )

Similarly, please note here we are only using the first 4 weeks of data in load_ts1 for model selection.

```{r message=FALSE, warning=FALSE}

   
###### Short period VS Long period VS Short-Long combination ######

### ***** adjust if loops, based on best values ***** ###

# forecast horizon:
h = 24*7

# 1:small, 2:large
# tables holding forecasts:
FC_1 <- array(NA,c(h,length(load_ts1)))
FC_2<- array(NA,c(h,length(load_ts2)))
FC_comb <- array(NA,c(h,length(load_ts1)))
colnames(FC_1) <- names(load_ts1)
colnames(FC_2) <- names(load_ts1)
colnames(FC_comb) <- names(load_ts1)

# choice array:
choice <- array(NA,length(load_ts1))

for (i in 1:length(load_ts1)) {
  
  # time series, train-validation split:
  #Only use the first 4 weeks of data for load_ts1
  y1 <- head(load_ts1[[i]],24*7*4)
  y2 <- load_ts2[[i]]
  y1t <- head(y1, length(y1)-h)
  y1v <- tail(y1, h)
  y2t <- head(y2, length(y1)-h)
  y2v <- tail(y2, h)
  
  # multi-seasonal time series format, train-validation split:
  y1ms <- msts(head(load_ts1[[i]],24*7*4),seasonal.periods = c(24,24*7))
  y1mst <- head(y1ms, length(y1ms)-h)
  y1msv <- tail(y1ms, h)

  y2ms <- msts(load_ts2[[i]],seasonal.periods = c(24,24*7))
  y2mst <- head(y2ms, length(y2ms)-h)
  y2msv <- tail(y2ms, h)
  
  x1<-rep(0:23,length(y1ms)/24)
  x1t <- head(x1, length(y1ms)-h)
  x1v <- tail(x1, h)
  
  x2<-rep(0:23,length(y2ms)/24)
  x2t <- head(x2, length(y2ms)-h)
  x2v <- tail(x2, h)
  
  best_forecast <- array(NA,3)
  period <-c("short","long","comb")
  
  # generate forecasts for 1-month and combination:
  if (best[i]=="msts") {
    # Multiple Seasonalities
    fit1 <- stlm(y1mst, lambda = 0)
    fit2 <- stlm(y2mst, lambda = 0)
    FC_1[,i] <- as.vector(forecast(fit1, h=h)$mean)
    FC_2[,i] <-as.vector(forecast(fit2, h=h)$mean)
    FC_comb[,i] <- 0.5*as.vector(forecast(fit1,h=h)$mean) + 
      0.5*as.vector(forecast(fit2,h=h)$mean)
  } else if (best[i]=="tbats") {
    # tbats
    fit1 <- tbats(log(y1mst),use.box.cox=F,use.trend = TRUE,
                  use.damped.trend = TRUE,xreg=x1t)
    fit2 <- tbats(log(y2mst),use.box.cox=F,use.trend = TRUE,
                  use.damped.trend = TRUE,xreg=x2t)
    FC_1[,i] <- as.vector(exp(forecast(fit1, h=h)$mean))
    FC_2[,i]<- as.vector(exp(forecast(fit2, h=h)$mean))
    FC_comb[,i] <- 0.5*as.vector(exp(forecast(fit1, h=h)$mean)) + 
      0.5*as.vector(exp(forecast(fit2, h=h)$mean))
  } else if (best[i]=="thief") {
    # thief
    fit1 <- thief(y1t,h=h,comb="struc", usemodel="ets")
    fit2 <- thief(y2t,h=h,comb="struc", usemodel="ets")
    FC_1[,i] <- as.vector(forecast(fit1, h=h)$mean)
    FC_2[,i] <-as.vector(forecast(fit2, h=h)$mean)
    FC_comb[,i] <- 0.5*as.vector(forecast(fit1,h=h)$mean) + 
      0.5*as.vector(forecast(fit2,h=h)$mean)    
  } else {
    # MAPA
    FC_1[,i] <- as.vector(mapasimple(y1t, comb = "w.mean", fh = h)$forecast)
    FC_2[,i] <-as.vector(mapasimple(y2t, comb = "w.mean", fh = h)$forecast)
    FC_comb[,i] <- 0.5*as.vector(mapasimple(y1t, comb = "w.mean", fh = h)$forecast) +
      -.5*as.vector(mapasimple(y2t, comb = "w.mean", fh = h)$forecast)
    
  }
  # put forecast in best_forecast array
  best_forecast[1] = accuracy(FC_1[,i],y1msv)[5] 
  best_forecast[2] = accuracy(FC_2[,i],y1msv)[5] 
  best_forecast[3] = accuracy(FC_comb[,i],y1msv)[5]
  # choose forecast based on MAPE value:
  choice[i]=period[which.min(best_forecast)]
}
```

Result

```{r}
(choice)
```

## Generate Forecast

```{r}

###### PRODUCE FORECASTS ######

### --- adjust if loops, based on choice values!! --- ###

h <- 24*8
results <- array(NA, c(h,length(load_ts1)))
colnames(results) <- names(load_ts1)


for (i in 1:dim(results)[2]) {
  y1 <- head(load_ts1[[i]],7*4*24)
  y2 <-load_ts2[[i]]
  
  y1ms <- msts( head(load_ts1[[i]],7*4*24),seasonal.periods = c(24,24*7))
  y2ms <- msts(load_ts2[[i]][1:3648],seasonal.periods = c(24,24*7))
  
  x1 <- rep(0:23,length(y1ms)/24)
  x2 <- rep(0:23,length(y2ms)/24)
  
  if (best[i] == "msts") {
    if (choice[i] == "short") {
      results[,i] <- as.vector(forecast(stlm(y1ms, lambda = 0),h=h)$mean)
    }
    else if (choice[i]== "long"){
      results[,i] <- as.vector(forecast(stlm(y2ms, lambda = 0),h=h)$mean)
    }
    else {
      results[,i] <- 0.5*as.vector(forecast(stlm(y1ms, lambda = 0),h=h)$mean) +
        0.5*as.vector(forecast(stlm(y2ms, lambda = 0),h=h)$mean)
    }
  }
  else if (best[i] == "tbats") {
    if (choice[i] == "short") {
      results[,i] <- as.vector(exp(forecast(tbats(log(y1ms),use.box.cox=F,use.trend = TRUE,
                                                  use.damped.trend = TRUE,xreg=x1),h=h)$mean))
    }
    else if (choice[i]== "long"){
      results[,i] <- as.vector(exp(forecast(tbats(log(y2ms),use.box.cox=F,use.trend = TRUE,
                                                  use.damped.trend = TRUE,xreg=x2),h=h)$mean))
    }
    else {
      results[,i] <- 0.5*as.vector(exp(forecast(tbats(log(y1ms),use.box.cox=F,use.trend = TRUE,
                                                      use.damped.trend = TRUE,xreg=x1),h=h)$mean)) +
        0.5*as.vector(exp(forecast(tbats(log(y2ms),use.box.cox=F,use.trend = TRUE,
                                             use.damped.trend = TRUE,xreg=x2),h=h)$mean))
    }
  }
  else if (best[i] == "thief") {
    if (choice[i] == "short") {
      results[,i] <- as.vector(forecast(thief(y1,h=h,comb="struc", usemodel="ets"),h=h)$mean)
    }
    else if (choice[i]== "long"){
      results[,i] <- as.vector(forecast(thief(y2,h=h,comb="struc", usemodel="ets"),h=h)$mean)
    }
    else {
      results[,i] <- 0.5*as.vector(forecast(thief(y1,h=h,comb="struc", usemodel="ets"),h=h)$mean) +
        0.5*as.vector(forecast(thief(y2,h=h,comb="struc", usemodel="ets"),h=h)$mean)
    }
  }
  else {
    if (choice[i] == "short") { # MAPA
      results[,i] <- as.vector(mapasimple(y1, comb = "w.mean", fh = h)$forecast)
    }
    else if (choice[i]== "long"){
      results[,i] <- as.vector(mapasimple(y2, comb = "w.mean", fh = h)$forecast)
    }
    else {
      results[,i] <- 0.5*as.vector(mapasimple(y1, comb = "w.mean", fh = h)$forecast) +
        0.5*as.vector(mapasimple(y2, comb = "w.mean", fh = h)$forecast)
    }
  }
}
```

## Validation Process

Here we first generate seasonal naive data which is the last 7 days available data

That is why I have inputted 36 days into load_ts1

First 28 days of load_ts1 is used for testing (the 4 weeks data)

Since there is 1 delay in the submission process ,the last 7 days (29th to 36th dates of load_ts1) will be used as validation data.

Meanwhile, for the same reason , (23th to 29th date of data) will be used as seasonal naive data

```{r}

snaive <-array(0,c(10,8*24))
for (i in 1:length(load_ts1)){
  #The last 7th day of the actual data
  snaive[i,] <- tail( head(load_ts1[[i]],7*4*24),8*24)
}

Validation_Data <-array(0,c(10,8*24))
for (i in 1:length(load_ts1)){
  Validation_Data[i,] <- tail(load_ts1[[i]],8*24)
}


MAPEs <-array(0,length(load_ts1))
MAEs <-array(0,length(load_ts1))
MAEs_Naive <-array(0,length(load_ts1))
RMAEs <-array(0,length(load_ts1))

for( i in 1:length(load_ts1)){
  
  MAPEs[i] <- accuracy(results[,i],Validation_Data[i,])[5] 
  MAEs[i] <- accuracy(results[,i],Validation_Data[i,])[3] 
  MAEs_Naive[i] <- accuracy(snaive[i,],Validation_Data[i,])[3]
  RMAEs[i] <- accuracy(results[,i],Validation_Data[i,])[3]/accuracy(snaive[i,],Validation_Data[i,])[3]
  
}
```

## Final Result



MAPEs:

```{r}
MAPEs
```

RMAEs:

```{r}
RMAEs
```
Mean of RMAEs: 
```{r message=FALSE}
mean(RMAEs)
```

